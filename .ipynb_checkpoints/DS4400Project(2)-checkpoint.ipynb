{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256f75c8",
   "metadata": {
    "id": "256f75c8"
   },
   "source": [
    "# DS 4400 Project\n",
    "## Jeremy Cui and Rachna Lewis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "22d61be3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22d61be3",
    "outputId": "fa3af0c6-6da8-423a-af0e-c57929e9b955"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeremycui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeremycui/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jeremycui/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "import scipy as scp\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a4e62",
   "metadata": {
    "id": "569a4e62"
   },
   "source": [
    "## Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d8d14e9c",
   "metadata": {
    "id": "d8d14e9c"
   },
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv('https://cdn-lfs.huggingface.co/datasets/juliensimon/autonlp-data-song-lyrics-demo/367571c0d07193ad0c8d577124d27076615bd98493fc1f5f785771496e673682')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "DglefV8deXx4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DglefV8deXx4",
    "outputId": "b2fde6fa-5567-4e46-9a14-d85c232a28c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Intro: Method Man w/ sample] + (Sunny valenti...\n",
       "1        [Sean Paul:]. Aye. It's Sean Paul 'long side. ...\n",
       "2        Beauty finds refuge in herself. Lovers wrapped...\n",
       "3        You've changed your tune. many times since we'...\n",
       "4        I got all these J's rolled up. And got all the...\n",
       "                               ...                        \n",
       "48488    Unforgettable face I can't think of her name. ...\n",
       "48489    Shawty swing my way put that ass all in my fac...\n",
       "48490    Born in caught out. Care out fear in. Gear in ...\n",
       "48491    I've got nothing to say today. I used my words...\n",
       "48492    Going far Getting nowhere. Going far The way y...\n",
       "Name: Lyric, Length: 48493, dtype: object"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df['Lyric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "TW4ELqMz167H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TW4ELqMz167H",
    "outputId": "7f091211-db71-4dc3-a5d1-1b1951be4ce5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48493, 2)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6006486d",
   "metadata": {
    "id": "6006486d"
   },
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('https://cdn-lfs.huggingface.co/datasets/juliensimon/autonlp-data-song-lyrics-demo/709af4b6afd2d6bfd890e558c61aba15068e4758417abfb5d86bdbd0397431da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "019M50Ut2Axe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "019M50Ut2Axe",
    "outputId": "7bfc5edf-aea6-4800-d6f9-0902acdd92fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5389, 2)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1016395f",
   "metadata": {
    "id": "1016395f"
   },
   "outputs": [],
   "source": [
    "stop_word = stopwords.words('english')\n",
    "stop_words = []\n",
    "for word in stop_word:\n",
    "    wo = re.sub(\"'\", \"\", word)\n",
    "    stop_words.append(wo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2ac14f0f",
   "metadata": {
    "id": "2ac14f0f"
   },
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "new_lyrics = []\n",
    "\n",
    "# removes punctuations from each sample\n",
    "for lyrics in lyrics_df['Lyric'].tolist():\n",
    "    \n",
    "    lyrics_n = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\", lyrics)\n",
    "    lyrics_n2 = lyrics_n.translate(str.maketrans('', '', string.punctuation))\n",
    "    lyrics_n3 = re.sub(' +', ' ', lyrics_n2)\n",
    "    lyrics_n4 = re.sub(r'[0-9]+', '', lyrics_n3)\n",
    "    \n",
    "    new_list_song = []\n",
    "    for word in lyrics_n.split():\n",
    "        if word.lower() not in stop_words:\n",
    "            new_list_song.append(word)\n",
    "            new_song = \" \".join(new_list_song)\n",
    "    \n",
    "    new_lyrics.append(new_song)\n",
    "    \n",
    "lyrics_df['Lyric'] = new_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "adf03165",
   "metadata": {
    "id": "adf03165"
   },
   "outputs": [],
   "source": [
    "# cleaning test data\n",
    "new_lyrics_ = []\n",
    "\n",
    "for lyrics in val_df['Lyric'].tolist():\n",
    "    \n",
    "    lyrics_n = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\", lyrics)\n",
    "    lyrics_n2 = lyrics_n.translate(str.maketrans('', '', string.punctuation))\n",
    "    lyrics_n3 = re.sub(' +', ' ', lyrics_n2)\n",
    "    lyrics_n4 = re.sub(r'[0-9]+', '', lyrics_n3)\n",
    "    \n",
    "    new_list_song = []\n",
    "    for word in lyrics_n4.split():\n",
    "        if word.lower() not in stop_words:\n",
    "            new_list_song.append(word)\n",
    "            new_song = \" \".join(new_list_song)\n",
    "    \n",
    "    new_lyrics_.append(new_song)\n",
    "    \n",
    "val_df['Lyric'] = new_lyrics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c185793a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c185793a",
    "outputId": "c0c0fe1c-3477-49c9-fe8f-9824e2d019af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rock           18993\n",
      "Pop            11162\n",
      "Hip Hop         8898\n",
      "Indie           5113\n",
      "Heavy Metal     2739\n",
      "Dance           1588\n",
      "Name: Genre, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "lyrics_df.rename(columns={'Genre0': 'Genre'}, inplace=True)\n",
    "lyrics_df\n",
    "print(lyrics_df['Genre'].value_counts())\n",
    "val_df.rename(columns={'Genre0': 'Genre'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "-v0adn8jhazO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "-v0adn8jhazO",
    "outputId": "db02b279-d9d0-4cf8-b3f4-191cd1fb1889"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nineteen came town called Summer Love burning ...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coat hat gone Ive really cant look little empt...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Use Body keep mine Yeah Im Durango number Take...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>met Burger King fell love soda machine took ca...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>end everything degenerate cultures elegy reape...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5384</th>\n",
       "      <td>see look face got ringworm Im sorry tell got r...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5385</th>\n",
       "      <td>Id move Rockferry Tomorrow Id build house baby...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5386</th>\n",
       "      <td>hear thunder hear raining force blows windows ...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>Moving moving Im supposed hold back keep movin...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388</th>\n",
       "      <td>ask change color hair ask need thirty two pair...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5389 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Lyric        Genre\n",
       "0     nineteen came town called Summer Love burning ...         Rock\n",
       "1     coat hat gone Ive really cant look little empt...         Rock\n",
       "2     Use Body keep mine Yeah Im Durango number Take...      Hip Hop\n",
       "3     met Burger King fell love soda machine took ca...         Rock\n",
       "4     end everything degenerate cultures elegy reape...  Heavy Metal\n",
       "...                                                 ...          ...\n",
       "5384  see look face got ringworm Im sorry tell got r...         Rock\n",
       "5385  Id move Rockferry Tomorrow Id build house baby...          Pop\n",
       "5386  hear thunder hear raining force blows windows ...  Heavy Metal\n",
       "5387  Moving moving Im supposed hold back keep movin...         Rock\n",
       "5388  ask change color hair ask need thirty two pair...          Pop\n",
       "\n",
       "[5389 rows x 2 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "6d2baecb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "6d2baecb",
    "outputId": "1a9abd7a-e491-4d43-c308-50274b84c1a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48493</td>\n",
       "      <td>48493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>48247</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>.</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>23</td>\n",
       "      <td>18993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Lyric  Genre\n",
       "count   48493  48493\n",
       "unique  48247      6\n",
       "top         .   Rock\n",
       "freq       23  18993"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a90c3579",
   "metadata": {
    "id": "a90c3579"
   },
   "outputs": [],
   "source": [
    "lemma_df = pd.DataFrame(columns= ['Lyric','Genre'])\n",
    "lemma_df['Genre'] = lyrics_df['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "9d1f893f",
   "metadata": {
    "id": "9d1f893f"
   },
   "outputs": [],
   "source": [
    "lemma_df_test = pd.DataFrame(columns= ['Lyric','Genre'])\n",
    "lemma_df_test['Genre'] = val_df['Genre']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fMvCuzpciYdG",
   "metadata": {
    "id": "fMvCuzpciYdG"
   },
   "source": [
    "Should we consider not lemmatizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "294d16a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "294d16a6",
    "outputId": "4af31ea7-b8ca-4971-f2dc-c19420b07dc7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+ . get butter . . . Aiyo one thing sure keep ...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>. Aye. It's Sean Paul 'long side. mandem call ...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beauty find refuge herself. Lovers wrap inside...</td>\n",
       "      <td>Indie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You've change tune. many time since we've met....</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get J's roll up. get drink pour up. buy bottle...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48488</th>\n",
       "      <td>Unforgettable face can't think name. It's edge...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48489</th>\n",
       "      <td>Shawty swing way put ass face. Round round hea...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48490</th>\n",
       "      <td>Born catch out. Care fear in. Gear heart out. ...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48491</th>\n",
       "      <td>I've get nothing say today. use word yesterday...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48492</th>\n",
       "      <td>Going far Getting nowhere. Going far way are. ...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48493 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Lyric        Genre\n",
       "0      + . get butter . . . Aiyo one thing sure keep ...      Hip Hop\n",
       "1      . Aye. It's Sean Paul 'long side. mandem call ...          Pop\n",
       "2      Beauty find refuge herself. Lovers wrap inside...        Indie\n",
       "3      You've change tune. many time since we've met....         Rock\n",
       "4      get J's roll up. get drink pour up. buy bottle...      Hip Hop\n",
       "...                                                  ...          ...\n",
       "48488  Unforgettable face can't think name. It's edge...  Heavy Metal\n",
       "48489  Shawty swing way put ass face. Round round hea...      Hip Hop\n",
       "48490  Born catch out. Care fear in. Gear heart out. ...         Rock\n",
       "48491  I've get nothing say today. use word yesterday...  Heavy Metal\n",
       "48492  Going far Getting nowhere. Going far way are. ...          Pop\n",
       "\n",
       "[48493 rows x 2 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatizing lyrics\n",
    "song_list_full = []\n",
    "for lyric in lyrics_df['Lyric'].to_list():\n",
    "    new_song_list = []\n",
    "    new_song = \"\"\n",
    "    for word in lyric.split():\n",
    "        lem = WordNetLemmatizer()\n",
    "        lemm = lem.lemmatize(word, pos='v')\n",
    "        new_song_list.append(lemm)\n",
    "        new_song = \" \".join(new_song_list)\n",
    "    song_list_full.append(new_song)\n",
    "lemma_df['Lyric'] = song_list_full\n",
    "lemma_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a7dc2de0",
   "metadata": {
    "id": "a7dc2de0"
   },
   "outputs": [],
   "source": [
    "# lemmatizing lyrics for testing\n",
    "song_list_full = []\n",
    "for lyric in val_df['Lyric'].to_list():\n",
    "    new_song_list = []\n",
    "    new_song = \"\"\n",
    "    for word in lyric.split():\n",
    "        lem = WordNetLemmatizer()\n",
    "        lemm = lem.lemmatize(word, pos='v')\n",
    "        new_song_list.append(lemm)\n",
    "        new_song = \" \".join(new_song_list)\n",
    "    song_list_full.append(new_song)\n",
    "lemma_df_test['Lyric'] = song_list_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "65772924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nineteen come town call Summer Love burn baby ...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coat hat go Ive really cant look little empty ...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Use Body keep mine Yeah Im Durango number Take...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meet Burger King fell love soda machine take c...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>end everything degenerate culture elegy reaper...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5384</th>\n",
       "      <td>see look face get ringworm Im sorry tell get r...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5385</th>\n",
       "      <td>Id move Rockferry Tomorrow Id build house baby...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5386</th>\n",
       "      <td>hear thunder hear rain force blow windows barr...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>Moving move Im suppose hold back keep move Pus...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388</th>\n",
       "      <td>ask change color hair ask need thirty two pair...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5389 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Lyric        Genre\n",
       "0     nineteen come town call Summer Love burn baby ...         Rock\n",
       "1     coat hat go Ive really cant look little empty ...         Rock\n",
       "2     Use Body keep mine Yeah Im Durango number Take...      Hip Hop\n",
       "3     meet Burger King fell love soda machine take c...         Rock\n",
       "4     end everything degenerate culture elegy reaper...  Heavy Metal\n",
       "...                                                 ...          ...\n",
       "5384  see look face get ringworm Im sorry tell get r...         Rock\n",
       "5385  Id move Rockferry Tomorrow Id build house baby...          Pop\n",
       "5386  hear thunder hear rain force blow windows barr...  Heavy Metal\n",
       "5387  Moving move Im suppose hold back keep move Pus...         Rock\n",
       "5388  ask change color hair ask need thirty two pair...          Pop\n",
       "\n",
       "[5389 rows x 2 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "589ac8d9",
   "metadata": {
    "id": "589ac8d9"
   },
   "outputs": [],
   "source": [
    "# Getting Rid of Instrumental Songs - to be safe, removed any song with 'instrumental' in lyrics - only 50 songs\n",
    "lemma_df_instrumentals = lemma_df[lemma_df['Lyric'].str.contains('Instrumental')]\n",
    "cond = lemma_df['Lyric'].isin(lemma_df_instrumentals['Lyric'])\n",
    "lemma_df.drop(lemma_df[cond].index, inplace = True)\n",
    "lemma_df\n",
    "\n",
    "lemma_df_test_instrumentals = lemma_df_test[lemma_df_test['Lyric'].str.contains('Instrumental')]\n",
    "cond2 = lemma_df_test['Lyric'].isin(lemma_df_test_instrumentals['Lyric'])\n",
    "lemma_df_test.drop(lemma_df_test[cond2].index, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7339a0a5",
   "metadata": {
    "id": "7339a0a5"
   },
   "outputs": [],
   "source": [
    "# Evening distribution of Genres as well as possible (maintaining 20,000 rows)\n",
    "hip_hop_df = lemma_df[lemma_df['Genre'] == 'Hip Hop'].sample(n=4000)\n",
    "rock_df = lemma_df[lemma_df['Genre'] == 'Rock'].sample(n=4000)\n",
    "pop_df = lemma_df[lemma_df['Genre'] == 'Pop'].sample(n=4000)\n",
    "indie_df = lemma_df[lemma_df['Genre'] == 'Indie'].sample(n=4000)\n",
    "dance_df = lemma_df[lemma_df['Genre'] == 'Dance']\n",
    "metal_df = lemma_df[lemma_df['Genre'] == 'Heavy Metal']\n",
    "\n",
    "dfs_concat = [hip_hop_df, rock_df, pop_df, indie_df, dance_df, metal_df]\n",
    "\n",
    "training_songs = pd.concat(dfs_concat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2b01c4a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "2b01c4a7",
    "outputId": "eb7e987a-1497-42a1-d63c-f1b99f64b181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hip Hop        4000\n",
      "Rock           4000\n",
      "Pop            4000\n",
      "Indie          4000\n",
      "Heavy Metal    2737\n",
      "Dance          1587\n",
      "Name: Genre, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20324</td>\n",
       "      <td>20324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20261</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>.</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>13</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Lyric    Genre\n",
       "count   20324    20324\n",
       "unique  20261        6\n",
       "top         .  Hip Hop\n",
       "freq       13     4000"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_songs['Genre'].value_counts())\n",
    "training_songs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c0025594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17355</th>\n",
       "      <td>Niggas want interview wanna ask feel. feel fee...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32718</th>\n",
       "      <td>Snoop Dogg. I'm republic party democratic aprt...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31706</th>\n",
       "      <td>Baby.... Ooh baby.... we.... gotta man love ni...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31240</th>\n",
       "      <td>Uh yeah. Members Volume. Volume 3 uh. P. Soul ...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36074</th>\n",
       "      <td>One time circle block. Lil' buddy get murder f...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48408</th>\n",
       "      <td>Hand fate move finger point you. knock feet go...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48456</th>\n",
       "      <td>Running darkness. Won't good flee. Stalking go...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48469</th>\n",
       "      <td>Safe shadows. Deep night. know you're near. yo...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48488</th>\n",
       "      <td>Unforgettable face can't think name. It's edge...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48491</th>\n",
       "      <td>I've get nothing say today. use word yesterday...</td>\n",
       "      <td>Heavy Metal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20324 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Lyric        Genre\n",
       "17355  Niggas want interview wanna ask feel. feel fee...      Hip Hop\n",
       "32718  Snoop Dogg. I'm republic party democratic aprt...      Hip Hop\n",
       "31706  Baby.... Ooh baby.... we.... gotta man love ni...      Hip Hop\n",
       "31240  Uh yeah. Members Volume. Volume 3 uh. P. Soul ...      Hip Hop\n",
       "36074  One time circle block. Lil' buddy get murder f...      Hip Hop\n",
       "...                                                  ...          ...\n",
       "48408  Hand fate move finger point you. knock feet go...  Heavy Metal\n",
       "48456  Running darkness. Won't good flee. Stalking go...  Heavy Metal\n",
       "48469  Safe shadows. Deep night. know you're near. yo...  Heavy Metal\n",
       "48488  Unforgettable face can't think name. It's edge...  Heavy Metal\n",
       "48491  I've get nothing say today. use word yesterday...  Heavy Metal\n",
       "\n",
       "[20324 rows x 2 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7904b",
   "metadata": {
    "id": "cbd7904b"
   },
   "source": [
    "### Obtaining Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bAkPpea5Kb7",
   "metadata": {
    "id": "9bAkPpea5Kb7"
   },
   "source": [
    "Parameters to adjust in TfidfVectorizer:\n",
    "  - min_df in TfidfVectorizer(): minimum number of documents a word must appear in to be in vocab\n",
    "  - ngram_range: n-gram to use (1, 1)->unigram, (1, 2)-> unigrams and bigrams, (2,2)-> bigrams\n",
    "  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "78_2bBTk-R4R",
   "metadata": {
    "id": "78_2bBTk-R4R"
   },
   "outputs": [],
   "source": [
    "number_songs = 5000\n",
    "sampled_training_df = training_songs #.sample(n=number_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce3711",
   "metadata": {
    "id": "d2ce3711"
   },
   "outputs": [],
   "source": [
    "# tf-idf to obtain features, encoding response (genre)\n",
    "vectorizer = TfidfVectorizer()\n",
    "# fit lyrics to TfidfVectorizer\n",
    "vectors = vectorizer.fit_transform(sampled_training_df['Lyric'].to_list())\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "dense_list = dense.tolist()\n",
    "training_df = pd.DataFrame(dense_list, columns=feature_names)\n",
    "le = LabelEncoder()\n",
    "training_df['Genre'] = le.fit_transform(sampled_training_df['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ocImWxLSMk5U",
   "metadata": {
    "id": "ocImWxLSMk5U"
   },
   "outputs": [],
   "source": [
    "# tf-idf to obtain features, encoding response (genre)\n",
    "vectorizer_mdf3 = TfidfVectorizer(min_df=3)\n",
    "# fit lyrics to TfidfVectorizer\n",
    "vectors = vectorizer_mdf3.fit_transform(sampled_training_df['Lyric'].to_list())\n",
    "feature_names_mdf3 = vectorizer_mdf3.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "dense_list = dense.tolist()\n",
    "training_df_mdf3 = pd.DataFrame(dense_list, columns=feature_names_mdf3)\n",
    "le = LabelEncoder()\n",
    "training_df_mdf3['Genre'] = le.fit_transform(sampled_training_df['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2u1l_Bnr5BiM",
   "metadata": {
    "id": "2u1l_Bnr5BiM"
   },
   "outputs": [],
   "source": [
    "# tf-idf to obtain features, encoding response (genre)\n",
    "vectorizer_mdf10 = TfidfVectorizer(min_df=10)\n",
    "# fit lyrics to TfidfVectorizer\n",
    "vectors = vectorizer_mdf10.fit_transform(sampled_training_df['Lyric'].to_list())\n",
    "feature_names_mdf10 = vectorizer_mdf10.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "dense_list = dense.tolist()\n",
    "training_df_mdf10 = pd.DataFrame(dense_list, columns=feature_names_mdf10)\n",
    "le = LabelEncoder()\n",
    "training_df_mdf10['Genre'] = le.fit_transform(sampled_training_df['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "qC_vXvFSH3pK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "qC_vXvFSH3pK",
    "outputId": "e26e4a63-01cc-4f86-c0f8-e96759160cf8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>zig</th>\n",
       "      <th>zion</th>\n",
       "      <th>zip</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20319</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20321</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20323</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20324 rows × 9824 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000   03   05   06   09   10  100  1000   11   12  ...  zig  zion  zip  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0   0.0  0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0   0.0  0.0   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0   0.0  0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0   0.0  0.0   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0   0.0  0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...   ...  ...  ...  ...  ...   ...  ...   \n",
       "20319  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0   0.0  0.0   \n",
       "20320  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0   0.0  0.0   \n",
       "20321  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0   0.0  0.0   \n",
       "20322  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0   0.0  0.0   \n",
       "20323  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...  0.0   0.0  0.0   \n",
       "\n",
       "       zodiac  zombie  zombies  zone  zoo  zoom  Genre  \n",
       "0         0.0     0.0      0.0   0.0  0.0   0.0      2  \n",
       "1         0.0     0.0      0.0   0.0  0.0   0.0      2  \n",
       "2         0.0     0.0      0.0   0.0  0.0   0.0      2  \n",
       "3         0.0     0.0      0.0   0.0  0.0   0.0      2  \n",
       "4         0.0     0.0      0.0   0.0  0.0   0.0      2  \n",
       "...       ...     ...      ...   ...  ...   ...    ...  \n",
       "20319     0.0     0.0      0.0   0.0  0.0   0.0      1  \n",
       "20320     0.0     0.0      0.0   0.0  0.0   0.0      1  \n",
       "20321     0.0     0.0      0.0   0.0  0.0   0.0      1  \n",
       "20322     0.0     0.0      0.0   0.0  0.0   0.0      1  \n",
       "20323     0.0     0.0      0.0   0.0  0.0   0.0      1  \n",
       "\n",
       "[20324 rows x 9824 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df_mdf10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "R40bFrNgITWN",
   "metadata": {
    "id": "R40bFrNgITWN"
   },
   "outputs": [],
   "source": [
    "number_songs = 5000\n",
    "sampled_test_df = lemma_df_test #.sample(n=number_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c6b9c24",
   "metadata": {
    "id": "0c6b9c24"
   },
   "outputs": [],
   "source": [
    "vectors_te = vectorizer.transform(sampled_test_df['Lyric'].to_list())\n",
    "# turns sparse vector to dense vector\n",
    "dense = vectors.todense()\n",
    "dense_list = dense.tolist()\n",
    "testing_df = pd.DataFrame(dense_list, columns=feature_names)\n",
    "testing_df['Genre'] = le.transform(sampled_test_df['Genre'])\n",
    "# Heavy Metal = 1\n",
    "# Hip-hop = 2\n",
    "# Indie = 3\n",
    "# Pop = 4\n",
    "# Rock = 5\n",
    "# Dance = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "u85_KfkkNRqO",
   "metadata": {
    "id": "u85_KfkkNRqO"
   },
   "outputs": [],
   "source": [
    "vectorizer_mdf3_te = vectorizer_mdf3.transform(sampled_test_df['Lyric'].to_list())\n",
    "# turns sparse vector to dense vector\n",
    "dense = vectorizer_mdf3_te.todense()\n",
    "dense_list = dense.tolist()\n",
    "testing_df_mdf3 = pd.DataFrame(dense_list, columns=feature_names_mdf3)\n",
    "testing_df_mdf3['Genre'] = le.transform(sampled_test_df['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "dv3U7vRSIOh9",
   "metadata": {
    "id": "dv3U7vRSIOh9"
   },
   "outputs": [],
   "source": [
    "vectorizer_mdf10_te = vectorizer_mdf10.transform(sampled_test_df['Lyric'].to_list())\n",
    "# turns sparse vector to dense vector\n",
    "dense = vectorizer_mdf10_te.todense()\n",
    "dense_list = dense.tolist()\n",
    "testing_df_mdf10 = pd.DataFrame(dense_list, columns=feature_names_mdf10)\n",
    "testing_df_mdf10['Genre'] = le.transform(sampled_test_df['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de62920",
   "metadata": {
    "id": "FDUSRrZhv1db"
   },
   "outputs": [],
   "source": [
    "vectorizer_mdf100_te = vectorizer_mdf100.transform(sampled_test_df['Lyric'].to_list())\n",
    "# turns sparse vector to dense vector\n",
    "dense = vectorizer_mdf100_te.todense()\n",
    "dense_list = dense.tolist()\n",
    "testing_df_mdf100 = pd.DataFrame(dense_list, columns=feature_names_mdf100)\n",
    "testing_df_mdf100['Genre'] = le.transform(sampled_test_df['Genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c6d21",
   "metadata": {
    "id": "b31c6d21"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b009aa3c",
   "metadata": {
    "id": "b009aa3c"
   },
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7799f",
   "metadata": {
    "id": "95d7799f"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q40MAol8Rg0_",
   "metadata": {
    "id": "Q40MAol8Rg0_"
   },
   "source": [
    " - penalty: default l2, can also use elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "IadNJQhwvgy5",
   "metadata": {
    "id": "IadNJQhwvgy5"
   },
   "outputs": [],
   "source": [
    "X_train_mdf100, y_train_mdf100 = training_df_mdf100.iloc[:,:-1], training_df_mdf100.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b38927d0",
   "metadata": {
    "id": "b38927d0"
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(multi_class='multinomial', random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8_WiFHGTUbi7",
   "metadata": {
    "id": "8_WiFHGTUbi7"
   },
   "source": [
    "If you don't specify the random_state in the code, then every time you run(execute) your code a new random value is generated and the train and test datasets would have different values each time.\n",
    "\n",
    "However, if a fixed value is assigned like random_state = 0 or 1 or 42 or any other integer then no matter how many times you execute your code the result would be the same .i.e, same values in train and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UByauCedKvZu",
   "metadata": {
    "id": "UByauCedKvZu"
   },
   "source": [
    "Had to increase max_iter (# of iterations) to allow log reg to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ekOxrmTGJ8U5",
   "metadata": {
    "id": "ekOxrmTGJ8U5"
   },
   "outputs": [],
   "source": [
    "logreg_model_mdf10 = LogisticRegression(multi_class='multinomial', random_state=1, max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8252q9EdKDHm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8252q9EdKDHm",
    "outputId": "a541d084-081d-4b89-f2a7-5429197240df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=300, multi_class='multinomial', random_state=1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_model_mdf10.fit(X_train_mdf10, y_train_mdf10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "za2mDGK8OMJ3",
   "metadata": {
    "id": "za2mDGK8OMJ3"
   },
   "outputs": [],
   "source": [
    "X_test_mdf3, y_test_mdf3 = testing_df_mdf3.iloc[:,:-1], testing_df_mdf3.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "VSBVg0yjOGnK",
   "metadata": {
    "id": "VSBVg0yjOGnK"
   },
   "outputs": [],
   "source": [
    "y_pred_mdf3 = logreg_model_mdf3.predict(X_test_mdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "yY5LU8miLjHC",
   "metadata": {
    "id": "yY5LU8miLjHC"
   },
   "outputs": [],
   "source": [
    "y_pred_mdf10 = logreg_model_mdf10.predict(X_test_mdf10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "NFWlaSFWJ1lP",
   "metadata": {
    "id": "NFWlaSFWJ1lP"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = training_df_mdf10.iloc[:,:-1], training_df_mdf10.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "Sv_tlZsETTCW",
   "metadata": {
    "id": "Sv_tlZsETTCW"
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(multi_class='multinomial', random_state=3, max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a7480da8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7480da8",
    "outputId": "a25f3073-fcd7-49ca-89b3-5e262c0ca710"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=300, multi_class='multinomial', random_state=3)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "WUrgOn55KaQu",
   "metadata": {
    "id": "WUrgOn55KaQu"
   },
   "outputs": [],
   "source": [
    "X_test, y_test = testing_df_mdf10.iloc[:,:-1], testing_df_mdf10.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c529ab92",
   "metadata": {
    "id": "c529ab92"
   },
   "outputs": [],
   "source": [
    "y_pred = logreg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "bd74f597",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd74f597",
    "outputId": "1eec8e15-1c5e-4b38-8d62-3b50378e5a3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4828226555246054\n",
      "0.41884275734751525\n",
      "0.4267781795226502\n",
      "0.40489981726391827\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(precision_score(y_test, y_pred, average='macro'))\n",
    "print(recall_score(y_test, y_pred, average='macro'))\n",
    "print(f1_score(y_test, y_pred, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "BdiOtM-iL7ay",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdiOtM-iL7ay",
    "outputId": "e6b41291-6662-4433-dea9-5a40e37ff9b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4456\n",
      "0.3830958244220432\n",
      "0.3794585255903875\n",
      "0.358538624262293\n"
     ]
    }
   ],
   "source": [
    "# Metrics for min_df=100 (minimum 100 documents the word must appear in)\n",
    "print(accuracy_score(y_test_mdf10, y_pred_mdf10))\n",
    "print(precision_score(y_test_mdf10, y_pred_mdf10, average='macro'))\n",
    "print(recall_score(y_test_mdf10, y_pred_mdf10, average='macro'))\n",
    "print(f1_score(y_test_mdf10, y_pred_mdf10, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "50f14bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_no_pen = LogisticRegression(penalty='none', multi_class='multinomial', random_state=3, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1ffa7575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremycui/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=500, multi_class='multinomial', penalty='none',\n",
       "                   random_state=3)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_no_pen.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2pB27cE4I0K0",
   "metadata": {
    "id": "2pB27cE4I0K0"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "lhgbHU50NWmy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhgbHU50NWmy",
    "outputId": "32deb63c-a267-4b9b-bb64-1cebcbc79983"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ii_vwPd6Phjq",
   "metadata": {
    "id": "ii_vwPd6Phjq"
   },
   "source": [
    "Talk about model being sparse, think of models that work best with sparse data, etc!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a29bb",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d3289178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4f11166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = training_df_mdf10.iloc[:,:-1], training_df_mdf10.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8fb3dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = testing_df_mdf10.iloc[:,:-1], testing_df_mdf10.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f23bc2",
   "metadata": {},
   "source": [
    "- Retrieve best features using feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b31572ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=10\n",
      "Accuracy score: 0.9906514465656366\n",
      "F1 score: 0.991085016078917\n",
      "F1 score: 0.9898728859218546\n",
      "F1 score: 0.9904663652211086\n",
      "AUC score: 0.9996305370264831\n",
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=10\n",
      "Accuracy score: 0.3736304549675023\n",
      "F1 score: 0.3261448307783626\n",
      "F1 score: 0.35443057769497166\n",
      "F1 score: 0.3143759746962954\n",
      "AUC score: 0.6788574208571445\n",
      "\n",
      "\n",
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=50\n",
      "Accuracy score: 0.9986715213540641\n",
      "F1 score: 0.9987486275551977\n",
      "F1 score: 0.9983546179656487\n",
      "F1 score: 0.9985509097100183\n",
      "AUC score: 0.9999644431611038\n",
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=50\n",
      "Accuracy score: 0.45255338904363973\n",
      "F1 score: 0.46735306991479947\n",
      "F1 score: 0.3961004668321792\n",
      "F1 score: 0.3741776212752313\n",
      "AUC score: 0.7463103255093936\n",
      "\n",
      "\n",
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=100\n",
      "Accuracy score: 0.998769927179689\n",
      "F1 score: 0.9989208793948997\n",
      "F1 score: 0.998432279780217\n",
      "F1 score: 0.9986752503102015\n",
      "AUC score: 0.9999649370379591\n",
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=100\n",
      "Accuracy score: 0.46555246053853294\n",
      "F1 score: 0.47958641901262283\n",
      "F1 score: 0.40021734722968666\n",
      "F1 score: 0.37951727225447573\n",
      "AUC score: 0.7633064378383415\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_of_trees = [10, 50, 100]\n",
    "\n",
    "# creates different-sized random forest classifiers using gini split\n",
    "# calculates accuracy, precision, recall, f1 score, and AUC metrics for each random forest\n",
    "# on testing and training data\n",
    "for tree_num in num_of_trees:\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=tree_num)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_tr = rf.predict(X_train)\n",
    "    y_pred_te = rf.predict(X_test)\n",
    "\n",
    "    # prints evaluation metrics on logistic regression model results\n",
    "    print(f'Accuracy, F1 score, and AUC for Random Forest on Training data tree size={tree_num}')\n",
    "    print('Accuracy score:', accuracy_score(y_train, y_pred_tr))\n",
    "    print('F1 score:', precision_score(y_train, y_pred_tr, average='macro'))\n",
    "    print('F1 score:', recall_score(y_train, y_pred_tr, average='macro'))\n",
    "    print('F1 score:', f1_score(y_train, y_pred_tr, average='macro'))\n",
    "    # AUC for random forest on training data\n",
    "    rf_probs_tr = rf.predict_proba(X_train)\n",
    "    print('AUC score:', roc_auc_score(y_train, rf_probs_tr, multi_class='ovr'))\n",
    "    \n",
    "    print(f'Accuracy, F1 score, and AUC for Random Forest on Training data tree size={tree_num}')\n",
    "    print('Accuracy score:', accuracy_score(y_test, y_pred_te))\n",
    "    print('F1 score:', precision_score(y_test, y_pred_te, average='macro'))\n",
    "    print('F1 score:', recall_score(y_test, y_pred_te, average='macro'))\n",
    "    print('F1 score:', f1_score(y_test, y_pred_te, average='macro'))\n",
    "    # AUC for random forest on training data\n",
    "    rf_probs_te = rf.predict_proba(X_test)\n",
    "    print('AUC score:', roc_auc_score(y_test, rf_probs_te, multi_class='ovr'))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "680df7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=10\n",
      "Accuracy score: 0.9898641999606377\n",
      "F1 score: 0.990645298452734\n",
      "F1 score: 0.9894019506353265\n",
      "F1 score: 0.9900081319093887\n",
      "AUC score: 0.999630959461621\n",
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=10\n",
      "Accuracy score: 0.3608170844939647\n",
      "F1 score: 0.3191282321805008\n",
      "F1 score: 0.34371984974748604\n",
      "F1 score: 0.30461230490838304\n",
      "AUC score: 0.6785637402564562\n",
      "\n",
      "\n",
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=50\n",
      "Accuracy score: 0.998769927179689\n",
      "F1 score: 0.9985412520260031\n",
      "F1 score: 0.9988123995029644\n",
      "F1 score: 0.9986764500848504\n",
      "AUC score: 0.9999641646735533\n",
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=50\n",
      "Accuracy score: 0.4343546889507892\n",
      "F1 score: 0.4446057202493729\n",
      "F1 score: 0.38692284694553597\n",
      "F1 score: 0.36038120126501244\n",
      "AUC score: 0.74355051358536\n",
      "\n",
      "\n",
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=100\n",
      "Accuracy score: 0.998769927179689\n",
      "F1 score: 0.9988123042373714\n",
      "F1 score: 0.9985397590983465\n",
      "F1 score: 0.9986756756609959\n",
      "AUC score: 0.9999655056507523\n",
      "Accuracy, F1 score, and AUC for Random Forest on Training data tree size=100\n",
      "Accuracy score: 0.43101207056638813\n",
      "F1 score: 0.43144855979054353\n",
      "F1 score: 0.3812370816357205\n",
      "F1 score: 0.3483769108030608\n",
      "AUC score: 0.7631844723043284\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_of_trees = [10, 50, 100]\n",
    "\n",
    "# creates different-sized random forest classifiers using entropy split\n",
    "# calculates accuracy, precision, recall, f1 score, and AUC metrics for each random forest\n",
    "# on testing and training data\n",
    "for tree_num in num_of_trees:\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=tree_num, criterion='entropy')\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_tr = rf.predict(X_train)\n",
    "    y_pred_te = rf.predict(X_test)\n",
    "\n",
    "    # prints evaluation metrics on logistic regression model results\n",
    "    print(f'Accuracy, F1 score, and AUC for Random Forest on Training data tree size={tree_num}')\n",
    "    print('Accuracy score:', accuracy_score(y_train, y_pred_tr))\n",
    "    print('F1 score:', precision_score(y_train, y_pred_tr, average='macro'))\n",
    "    print('F1 score:', recall_score(y_train, y_pred_tr, average='macro'))\n",
    "    print('F1 score:', f1_score(y_train, y_pred_tr, average='macro'))\n",
    "    # AUC for random forest on training data\n",
    "    rf_probs_tr = rf.predict_proba(X_train)\n",
    "    print('AUC score:', roc_auc_score(y_train, rf_probs_tr, multi_class='ovr'))\n",
    "    \n",
    "    print(f'Accuracy, F1 score, and AUC for Random Forest on Training data tree size={tree_num}')\n",
    "    print('Accuracy score:', accuracy_score(y_test, y_pred_te))\n",
    "    print('F1 score:', precision_score(y_test, y_pred_te, average='macro'))\n",
    "    print('F1 score:', recall_score(y_test, y_pred_te, average='macro'))\n",
    "    print('F1 score:', f1_score(y_test, y_pred_te, average='macro'))\n",
    "    # AUC for random forest on training data\n",
    "    rf_probs_te = rf.predict_proba(X_test)\n",
    "    print('AUC score:', roc_auc_score(y_test, rf_probs_te, multi_class='ovr'))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fca2c3",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "1ad5f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "077160e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for AdaBoostClassifier Training Data boost size=10\n",
      "Accuracy score: 0.998769927179689\n",
      "F1 score: 0.9984601785921372\n",
      "F1 score: 0.9988949800462089\n",
      "F1 score: 0.9986767472599819\n",
      "AUC score: 0.9999992662375545\n",
      "Evaluation Metrics for AdaBoostClassifier Testing Data boost size=10\n",
      "Accuracy score: 0.3665738161559889\n",
      "F1 score: 0.3253368391239475\n",
      "F1 score: 0.33240335258678305\n",
      "F1 score: 0.3125093427902424\n",
      "AUC score: 0.6748303259216453\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [230]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# what max depth for the decision tree base?\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ada_ \u001b[38;5;241m=\u001b[39m AdaBoostClassifier(sb_dt, n_estimators\u001b[38;5;241m=\u001b[39mbases)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mada_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# predicts y from x_train and x_test\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ypred_train \u001b[38;5;241m=\u001b[39m ada_\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:486\u001b[0m, in \u001b[0;36mAdaBoostClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:145\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    141\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iboost \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:548\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03mPerform a single boost according to the real multi-class SAMME.R\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost_real\u001b[49m\u001b[43m(\u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:559\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    555\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    557\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m--> 559\u001b[0m y_predict_proba \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iboost \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py:971\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03m\"\"\"Predict class probabilities of the input samples X.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \n\u001b[1;32m    949\u001b[0m \u001b[38;5;124;03mThe predicted class probability is the fraction of samples of the same\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;124;03m    classes corresponds to that in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    970\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 971\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py:433\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_input:\n\u001b[0;32m--> 433\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    435\u001b[0m         X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[1;32m    436\u001b[0m     ):\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    797\u001b[0m         )\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 800\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    803\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:103\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space np.isfinite to prevent\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# false positives from overflow in sum method. The sum is also calculated\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# safely to reduce dtype induced overflows.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m is_float \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_float \u001b[38;5;129;01mand\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(\u001b[43m_safe_accumulator_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_float:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py:893\u001b[0m, in \u001b[0;36m_safe_accumulator_op\u001b[0;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;124;03mThis function provides numpy accumulator functions with a float64 dtype\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;124;03mwhen used on a floating point input. This prevents accumulator overflow on\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    The output of the accumulator function passed to this function.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(x\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[0;32m--> 893\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m     result \u001b[38;5;241m=\u001b[39m op(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2259\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2256\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_of_bases = [10, 50, 100]\n",
    "\n",
    "for bases in num_of_bases:\n",
    "    \n",
    "    # initializes decision tree classifier base\n",
    "    sb_dt = tree.DecisionTreeClassifier()\n",
    "    # what max depth for the decision tree base?\n",
    "    ada_ = AdaBoostClassifier(sb_dt, n_estimators=bases)\n",
    "    ada_.fit(X_train, y_train)\n",
    "    \n",
    "    # predicts y from x_train and x_test\n",
    "    ypred_train = ada_.predict(X_train)\n",
    "    ypred_test = ada_.predict(X_test)\n",
    "\n",
    "    # prints accuracy, F1 score, and AUC for AdaBoostClassifier on training data\n",
    "    print(f'Evaluation Metrics for AdaBoostClassifier Training Data boost size={bases}')\n",
    "    print('Accuracy score:', accuracy_score(y_train, ypred_train))\n",
    "    print('F1 score:', precision_score(y_train, ypred_train, average='macro'))\n",
    "    print('F1 score:', recall_score(y_train, ypred_train, average='macro'))\n",
    "    print('F1 score:', f1_score(y_train, ypred_train, average='macro'))\n",
    "    # AUC for random forest on training data\n",
    "    ada_probs_tr = ada_.predict_proba(X_train)\n",
    "    print('AUC score:', roc_auc_score(y_train, ada_probs_tr, average='macro', multi_class='ovr'))\n",
    "    # prints accuracy, F1 score, and AUC for AdaBoostClassifier on testing data\n",
    "    print(f'Evaluation Metrics for AdaBoostClassifier Testing Data boost size={bases}')\n",
    "    print('Accuracy score:', accuracy_score(y_test, ypred_test))\n",
    "    print('F1 score:', precision_score(y_test, ypred_test, average='macro'))\n",
    "    print('F1 score:', recall_score(y_test, ypred_test, average='macro'))\n",
    "    print('F1 score:', f1_score(y_test, ypred_test, average='macro'))\n",
    "    # AUC for random forest on testing data\n",
    "    ada_probs_te = ada_.predict_proba(X_test)\n",
    "    print('AUC score:', roc_auc_score(y_test, ada_probs_te, average='macro', multi_class='ovr'))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63945b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS4120Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
